"""Rescore hypotheses based on extra models.

Here is an example command to run rescorer. It requires
a hypotheses generated by generate.py
./rescorer.par --l2r-model-path fwm.pt --l2r-model-weight 2
--reverse-model-path bwm.pt --lm-model-path lm.pt
--translation-info-export-path hypos.p
"""
from __future__ import absolute_import, division, print_function, unicode_literals

import argparse
import pickle
import queue
from enum import Enum

import torch
from fairseq import bleu
from pytorch_translate import hybrid_transformer_rnn  # noqa
from pytorch_translate import utils
from pytorch_translate.rescoring.model_scorers import (
    LMScorer,
    R2LModelScorer,
    ReverseModelScorer,
    SimpleModelScorer,
)
from pytorch_translate.tasks import pytorch_translate_task  # noqa
from tqdm import tqdm


class FeatureList(Enum):
    L2R_MODEL_SCORE = 0
    R2L_MODEL_SCORE = 1
    REVERSE_MODEL_SCORE = 2
    LM_SCORE = 3


class Rescorer:
    """Reranks n-best hypotheses based on extra models and parameters"""

    def __init__(self, args):
        self.args = args

        assert (
            args.l2r_model_path is not None
        ), "Rescoring needs --l2r-model-path which generated given hypotheses"
        self.l2r_model_scorer = SimpleModelScorer(args, args.l2r_model_path)
        self.forward_task = self.l2r_model_scorer.task

        self.r2l_model_scorer = None
        if args.r2l_model_path:
            self.r2l_model_scorer = R2LModelScorer(args, args.r2l_model_path)

        self.reverse_model_scorer = None
        if args.reverse_model_path:
            self.reverse_model_scorer = ReverseModelScorer(
                args, args.reverse_model_path, self.forward_task
            )

        self.lm_scorer = None
        if args.lm_model_path:
            self.lm_scorer = LMScorer(args, args.lm_model_path, self.forward_task)

    def combine_weighted_scores(self, scores, src_tokens, hypos):
        """combine scores from different models"""
        src_len = torch.tensor(len(src_tokens), dtype=torch.float)
        tgt_len = torch.tensor(
            [len(hypo["tokens"]) for hypo in hypos], dtype=torch.float
        )

        scores[
            :, FeatureList.L2R_MODEL_SCORE.value
        ] *= (
            self.args.l2r_model_weight
        )  # L2R model score should be length normalized already
        scores[:, FeatureList.R2L_MODEL_SCORE.value] *= (
            self.args.r2l_model_weight / tgt_len
        )
        scores[:, FeatureList.REVERSE_MODEL_SCORE.value] *= (
            self.args.reverse_model_weight / src_len
        )
        scores[:, FeatureList.LM_SCORE.value] *= self.args.lm_model_weight / src_len
        return scores.sum(dim=1).max(0)[1]

    def score(self, src_tokens, hypos):
        """run models and compute scores based on p(y), p(x|y) etc."""
        scores = torch.zeros((len(hypos), len(FeatureList)), dtype=torch.float)

        self.compute_l2r_model_scores(src_tokens, hypos, scores)
        self.compute_r2l_model_scores(src_tokens, hypos, scores)
        self.compute_reverse_model_scores(src_tokens, hypos, scores)
        self.compute_lm_scores(src_tokens, hypos, scores)

        max_score_index = self.combine_weighted_scores(scores, src_tokens, hypos)
        return hypos[max_score_index]["tokens"].int().cpu()

    def compute_l2r_model_scores(self, src_tokens, hypos, scores):
        for i, hypo in enumerate(hypos):
            scores[i, FeatureList.L2R_MODEL_SCORE.value] = hypo["score"]

    def compute_r2l_model_scores(self, src_tokens, hypos, scores):
        if not self.r2l_model_scorer:
            return
        r2l_scores = self.r2l_model_scorer.score(src_tokens, hypos)
        scores[:, FeatureList.R2L_MODEL_SCORE.value] = r2l_scores[:]

    def compute_reverse_model_scores(self, src_tokens, hypos, scores):
        """computes p(x|y) for each hypothesis. """
        if not self.reverse_model_scorer:
            return

        scores[
            :, FeatureList.REVERSE_MODEL_SCORE.value
        ] = self.reverse_model_scorer.score(src_tokens, hypos)

    def compute_lm_scores(self, src_tokens, hypos, scores):
        """computes p(x|y) for each hypothesis. """
        if not self.lm_scorer:
            return

        lm_scores = self.lm_scorer.score(src_tokens, hypos)
        scores[:, FeatureList.LM_SCORE.value] = lm_scores[:]


def get_arg_parser():
    parser = argparse.ArgumentParser(
        description=("Rescore generated hypotheses with extra models")
    )
    parser.add_argument(
        "--translation-info-export-path",
        default=None,
        type=str,
        help=("Optional path to save translation info output in pickled format"),
    )
    parser.add_argument(
        "--l2r-model-path",
        default=None,
        type=str,
        help=("Provide a path for the l2r rescoring model"),
    )
    parser.add_argument(
        "--l2r-model-weight",
        default=1.0,
        type=float,
        help=("Provide a weight for the l2r rescoring model"),
    )
    parser.add_argument(
        "--r2l-model-path",
        default=None,
        type=str,
        help=("Provide a path for the r2l rescoring model"),
    )
    parser.add_argument(
        "--r2l-model-weight",
        default=1.0,
        type=float,
        help=("Provide a weight for the r2l rescoring model"),
    )
    parser.add_argument(
        "--reverse-model-path",
        default=None,
        type=str,
        help=("Provide a path for the reverse rescoring model"),
    )
    parser.add_argument(
        "--reverse-model-weight",
        default=1.0,
        type=float,
        help=("Provide a weight for the reverse rescoring model"),
    )
    parser.add_argument(
        "--lm-model-path",
        default=None,
        type=str,
        help=("Provide a path for the language model rescoring model"),
    )
    parser.add_argument(
        "--lm-model-weight",
        default=1.0,
        type=float,
        help=("Provide a weight for the lm rescoring model"),
    )
    return parser


def main():
    """ Run rescoring on multiple GPU's. Computes BLEU score on rescored hypotheses.
    """
    args = get_arg_parser().parse_args()
    assert (
        args.translation_info_export_path is not None
    ), "--translation_info_export_path is required for rescoring"

    _, _, task = utils.load_diverse_ensemble_for_inference([args.l2r_model_path])
    pad = task.tgt_dict.pad()
    eos = task.tgt_dict.eos()
    unk = task.tgt_dict.unk()

    base_bleu_scorer = bleu.Scorer(pad, eos, unk)
    rescoring_bleu_scorer = bleu.Scorer(pad, eos, unk)

    num_workers = torch.cuda.device_count()
    output_queue = torch.multiprocessing.get_context("spawn").Queue()
    spawn_context = torch.multiprocessing.spawn(
        fn=run_single,
        args=(num_workers, args, output_queue),
        nprocs=num_workers,
        join=False,
    )

    count = 0
    while not spawn_context.join(timeout=0):
        try:
            while True:
                target_tokens = output_queue.get_nowait()
                base_bleu_scorer.add(
                    torch.tensor(target_tokens["target_tokens"]),
                    torch.tensor(target_tokens["top_tokens"]),
                )
                rescoring_bleu_scorer.add(
                    torch.tensor(target_tokens["target_tokens"]),
                    torch.tensor(target_tokens["rescoring_top_tokens"]),
                )
                count += 1
        except queue.Empty:
            pass

    print("| Number of scored sentences: ", count)
    print("| Base ", base_bleu_scorer.result_string())
    print("| Rescoring ", rescoring_bleu_scorer.result_string())


def run_single(rank=0, num_workers=1, args=None, output_queue=None):
    """ Load hypotheses from --translation-info-export-path and score different
    parts of data based on num_workers. Append results to output_queue.
    """
    if torch.cuda.is_available():
        torch.cuda.set_device(rank)

    rescorer = Rescorer(args)

    f = open(args.translation_info_export_path, "rb")
    trans_info_list = pickle.load(f)
    f.close()

    for i in tqdm(range(rank, len(trans_info_list), num_workers)):
        # Move tokens to cuda before rescoring
        trans_info_list[i]["hypos"] = [
            {"score": hypo["score"], "tokens": hypo["tokens"].cuda()}
            for hypo in trans_info_list[i]["hypos"]
        ]

        rescoring_top_tokens = rescorer.score(
            trans_info_list[i]["src_tokens"].cuda(), trans_info_list[i]["hypos"]
        )

        target_tokens = {
            "target_tokens": trans_info_list[i]["target_tokens"].int().cpu().numpy(),
            "top_tokens": trans_info_list[i]["hypos"][0]["tokens"].int().cpu().numpy(),
            "rescoring_top_tokens": rescoring_top_tokens.int().cpu().numpy(),
        }
        output_queue.put_nowait(target_tokens)


if __name__ == "__main__":
    main()
